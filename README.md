# youtube-chatbot-backend

A small backend service that ingests YouTube video transcripts into a vector database and exposes a simple API to query those transcripts using an LLM.

## Key features

- Load a YouTube video transcript, split it into chunks, and store embeddings in Qdrant
- Use Google Generative AI embeddings (Gemini) to embed transcript chunks
- Query a loaded video's transcript and get an LLM-generated answer based on the most relevant chunks
- Built with FastAPI for simple HTTP endpoints

## API Endpoints

- GET / -> health check
- POST /api/load_video -> body: { "video_link": "<youtube url>" }
  - Extracts video id, fetches transcript, splits text, and adds documents to Qdrant collection for that video.
- POST /api/query -> body: { "query": "<question>", "video_id": "<video_id>" }
  - Returns an answer generated by the LLM using the top retrieved chunks from the vector store.

## Quickstart (local, Windows PowerShell)

1. Create and activate a virtual environment

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

2. Install requirements

```powershell
pip install -r requirements.txt
```

3. Populate `.env` with required variables (see below)

4. Run the app

```powershell
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

## Important environment variables

- GOOGLE_API_KEY - Google API key for embeddings / LLM
- QDRANT_URL - Qdrant cloud URL
- QDRANT_API_KEY - Qdrant API key

Keep `.env` out of source control and never commit secrets. The repository currently includes an example `.env` location at the project root.

## Project layout

- `main.py` - FastAPI app and endpoints
- `core/` - core utilities
  - `youtube_loader.py` - fetches YouTube captions/transcripts
  - `text_splitter.py` - splits transcript into chunks
  - `vectorstore.py` - Qdrant vector store wiring
  - `chat_response.py` - builds prompt, retrieves context, queries LLM
- `models/` - Pydantic request models
